### üõ°Ô∏è Redund√¢ncia de Dados H√≠brida: SQL Server On-Premises para Azure Data Lake

**Azure Data Factory ‚Ä¢ Integra√ß√£o H√≠brida ‚Ä¢ Governan√ßa de Dados ‚Ä¢ DataOps ‚Ä¢ Continuidade de Neg√≥cio (BCP)**


![Azure_Databricks01](https://github.com/user-attachments/assets/8ddea732-e045-4694-9207-87aeb9403938)


**Bootcamp Microsoft AI for Tech - Azure Databricks**

---



üìå **Vis√£o Geral**

Este projeto implementa uma arquitetura de redund√¢ncia de dados em ambiente h√≠brido, com foco em Continuidade de Neg√≥cio (BCP), governan√ßa e conformidade.

A solu√ß√£o realiza a ingest√£o segura de dados de um SQL Server on-premises para o Azure Data Lake Storage Gen2 (ADLS), utilizando uma arquitetura em camadas (Raw e Bronze).
A orquestra√ß√£o √© feita via Azure Data Factory (ADF) com Self-hosted Integration Runtime (SHIR), reproduzindo cen√°rios reais de grandes organiza√ß√µes que operam sistemas legados cr√≠ticos e precisam integrar dados √† nuvem sem expor a infraestrutura interna.


---

üéØ **Problema de Neg√≥cio que o Projeto Resolve**

Em ambientes altamente regulados (bancos, seguradoras, setor p√∫blico), √© comum encontrar os seguintes desafios:

‚Ä¢ Depend√™ncia excessiva de infraestrutura local, sem redund√¢ncia geogr√°fica

‚Ä¢ Risco de indisponibilidade operacional, causado por falhas f√≠sicas ou l√≥gicas

‚Ä¢ Dificuldade de integrar dados legados √† nuvem, devido a firewalls e pol√≠ticas de seguran√ßa

‚Ä¢ Exig√™ncias regulat√≥rias relacionadas √† rastreabilidade, auditoria e recupera√ß√£o de desastres


Com base na minha trajet√≥ria de 15+ anos atuando em sistemas cr√≠ticos banc√°rios (Bradesco), projetei este fluxo para garantir que os dados sejam:

‚Ä¢ Replicados de forma segura

‚Ä¢ Armazenados com linhagem preservada

‚Ä¢ Prontos para recupera√ß√£o em cen√°rios de conting√™ncia

‚Ä¢ Aderentes a boas pr√°ticas de governan√ßa e compliance



---

üéØ **Objetivos do Projeto**

‚Ä¢ Implementar pipelines de redund√¢ncia h√≠brida usando Azure Data Factory

‚Ä¢ Demonstrar dom√≠nio sobre Self-hosted Integration Runtime (SHIR)

‚Ä¢ Estruturar dados em camadas Raw e Bronze, preservando o dado original

‚Ä¢ Automatizar a promo√ß√£o de dados utilizando Azure Databricks (PySpark)

‚Ä¢ Aplicar pr√°ticas de seguran√ßa, rastreabilidade e DataOps



---

üõ†Ô∏è **Decis√µes T√©cnicas & Justificativas (Trade-offs)**

üîπ Self-hosted Integration Runtime (SHIR)

Escolhido por ser o padr√£o corporativo para integra√ß√£o on-premises ‚Üí cloud, permitindo:

‚Ä¢ Tr√°fego seguro de dentro para fora

‚Ä¢ Nenhuma exposi√ß√£o do banco √† internet p√∫blica

‚Ä¢ Ader√™ncia a pol√≠ticas de firewall corporativo


üîπ **Azure Key Vault**

Centraliza credenciais e strings de conex√£o, evitando:

‚Ä¢ Segredos hardcoded

‚Ä¢ Risco operacional e falhas de compliance


üîπ **Arquitetura em Camadas (Raw ‚Üí Bronze)**

‚Ä¢ Raw: preserva o dado original, sem transforma√ß√£o (auditoria e replay)

‚Ä¢ Bronze: organiza√ß√£o m√≠nima para consumo anal√≠tico

‚Ä¢ Facilita governan√ßa, debugging e evolu√ß√£o do pipeline


üîπ **TXT/CSV vs Parquet**

TXT/CSV na camada Raw para **fidelidade ao dado original**


Processamento posterior no Databricks preparando o dado para formatos anal√≠ticos



---

üöÄ **Tecnologias Utilizadas**

‚Ä¢ **Orquestra√ß√£o:** Azure Data Factory (ADF)

‚Ä¢ **Integra√ß√£o H√≠brida:** Self-hosted Integration Runtime (SHIR)

‚Ä¢ **Armazenamento:** Azure Data Lake Storage Gen2

‚Ä¢ **Seguran√ßa:** Azure Key Vault

‚Ä¢ **Processamento:** Azure Databricks (PySpark)

‚Ä¢ **Fonte de Dados:** SQL Server (On-Premises)

‚Ä¢ **Versionamento:** GitHub



---

üñ•Ô∏è **Requisitos de Hardware e Software**

**Hardware (On-Premises)**

‚Ä¢ M√°quina dedicada para o Self-hosted IR

‚Ä¢ M√≠nimo recomendado:

‚Ä¢ 8 GB de RAM

‚Ä¢ 2 vCPUs



**Software**

‚Ä¢ Windows Server ou Windows 10+

‚Ä¢ SQL Server (Express ou superior)

‚Ä¢ Azure CLI

‚Ä¢ Navegador moderno


**Recursos Azure**

‚Ä¢ Azure Data Factory

‚Ä¢ Azure Data Lake Storage Gen2

‚Ä¢ Azure Key Vault

‚Ä¢ Azure Databricks



---

üìÇ Estrutura do Reposit√≥rio

.
‚îú‚îÄ‚îÄ adf/
‚îÇ   ‚îú‚îÄ‚îÄ pipelines/        # Pipelines de ingest√£o SQL ‚Üí ADLS
‚îÇ   ‚îú‚îÄ‚îÄ datasets/         # Defini√ß√µes de origem e destino
‚îÇ   ‚îî‚îÄ‚îÄ linkedServices/   # Conex√µes com SQL, ADLS e Key Vault
‚îÇ
‚îú‚îÄ‚îÄ databricks/
‚îÇ   ‚îî‚îÄ‚îÄ notebooks/        # PySpark para promo√ß√£o Raw ‚Üí Bronze
‚îÇ
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ arquitetura/      # Diagramas da solu√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ imagens/          # Prints do ADF
‚îÇ   ‚îî‚îÄ‚îÄ guia_instalacao_ir.md
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ sql/              # DDL e dados de exemplo
‚îÇ   ‚îî‚îÄ‚îÄ powershell/       # Automa√ß√£o do Self-hosted IR
‚îÇ
‚îî‚îÄ‚îÄ README.md


---

‚ñ∂Ô∏è **Como Executar o Projeto**

> ‚ö†Ô∏è **Importante:**
Este projeto simula um **ambiente h√≠brido real.**
N√£o √© um projeto ‚Äúclone & run‚Äù e exige recursos Azure provisionados.



## Passo a passo resumido

**1. Configurar o ambiente on-premises**

‚Ä¢ Instalar SQL Server

‚Ä¢ Seguir o guia docs/guia_instalacao_ir.md para o SHIR



**2. Provisionar recursos Azure**

‚Ä¢ Data Factory

‚Ä¢ Data Lake Gen2

‚Ä¢ Key Vault

‚Ä¢ Databricks



**3. Configurar o Azure Data Factory**

‚Ä¢ Importar Linked Services

‚Ä¢ Importar Datasets

‚Ä¢ Importar Pipelines



**4. Executar o pipeline**

‚Ä¢ Execu√ß√£o manual ou via Trigger

‚Ä¢ Validar arquivos na camada Raw e Bronze



**5. Processamento com Databricks**
   

‚Ä¢ Executar notebooks PySpark para promo√ß√£o dos dados





---

üß† **Principais Aprendizados**

‚Ä¢ Integra√ß√£o segura entre ambientes h√≠bridos

‚Ä¢ Governan√ßa e preserva√ß√£o do dado original

‚Ä¢ Arquitetura orientada √† continuidade de neg√≥cio

‚Ä¢ Uso do Spark para engenharia de promo√ß√£o de dados



---

üîÆ **Pr√≥ximos Passos**

‚Ä¢ Implementar Incremental Load (Delta)

‚Ä¢ Integrar com Microsoft Purview

‚Ä¢ Criar CI/CD para Data Factory com Azure DevOps






---

üèÅ Conclus√£o

Este projeto demonstra uma solu√ß√£o realista, segura e governada para redund√¢ncia de dados em ambientes h√≠bridos, refletindo desafios encontrados em grandes corpora√ß√µes e traduzindo experi√™ncia em sistemas cr√≠ticos para pr√°ticas modernas de engenharia de dados em cloud.


---







---

## üìÇ Estrutura de Pastas e Arquivos 


<img width="805" height="1587" alt="Screenshot_20251111-165706" src="https://github.com/user-attachments/assets/e3d4e52f-41ab-41d3-9996-1bd4865d7def" />


---






 



---
**Autor:**
  Sergio Santos 

---

**Contato:**

[![Portf√≥lio S√©rgio Santos](https://img.shields.io/badge/Portf√≥lio-S√©rgio_Santos-111827?style=for-the-badge&logo=githubpages&logoColor=00eaff)](https://santosdevbjj.github.io/portfolio/)
[![LinkedIn S√©rgio Santos](https://img.shields.io/badge/LinkedIn-S√©rgio_Santos-0A66C2?style=for-the-badge&logo=linkedin&logoColor=white)](https://linkedin.com/in/santossergioluiz) 


---


