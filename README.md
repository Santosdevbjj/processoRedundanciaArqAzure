# Criando Processos de RedundÃ¢ncia de Arquivos na Azure.


![Azure_Databricks01](https://github.com/user-attachments/assets/8ddea732-e045-4694-9207-87aeb9403938)



---


# ğŸš€ Processo de RedundÃ¢ncia de Arquivos na Azure

Este projeto demonstra como implementar um **processo de redundÃ¢ncia de arquivos** utilizando **Azure Data Factory**, **Self-hosted Integration Runtime**, **Azure Data Lake Storage Gen2** e **Databricks**.  
O objetivo Ã© copiar dados de um **SQL Server on-premises** para o **Data Lake**, convertendo-os em arquivos `.txt/.csv` organizados por camadas (`raw` e `bronze`), garantindo redundÃ¢ncia, escalabilidade e boas prÃ¡ticas de integraÃ§Ã£o hÃ­brida.

---

## ğŸ“Œ Objetivos do Projeto
- Criar pipelines no **Azure Data Factory** para mover dados de SQL on-premises para o Data Lake.  
- Configurar **Self-hosted Integration Runtime (IR)** para conectar ambientes locais ao Azure.  
- Organizar dados em camadas (`raw` e `bronze`) para redundÃ¢ncia e governanÃ§a.  
- Documentar e versionar todos os artefatos no GitHub.  
- Demonstrar boas prÃ¡ticas de seguranÃ§a, parametrizaÃ§Ã£o e monitoramento.  

---

## ğŸ–¥ï¸ Tecnologias Utilizadas
- **Azure Data Factory (ADF)** â†’ OrquestraÃ§Ã£o de pipelines.  
- **Self-hosted Integration Runtime (IR)** â†’ ConexÃ£o segura com SQL on-premises.  
- **Azure Data Lake Storage Gen2 (ADLS)** â†’ Armazenamento em camadas.  
- **Azure Key Vault** â†’ Gerenciamento seguro de segredos.  
- **Databricks** â†’ Processamento e promoÃ§Ã£o de dados da camada `raw` para `bronze`.  
- **SQL Server** â†’ Base de dados on-premises.  
- **GitHub** â†’ Versionamento e documentaÃ§Ã£o.  

---

## ğŸ“‚ Estrutura de Pastas e Arquivos 


<img width="805" height="1587" alt="Screenshot_20251111-165706" src="https://github.com/user-attachments/assets/e3d4e52f-41ab-41d3-9996-1bd4865d7def" />





---

## ğŸ“‘ ExplicaÃ§Ã£o dos Arquivos

### ğŸ“ docs/
- **imagens/adf_linked_services.png** â†’ Print da configuraÃ§Ã£o dos Linked Services no ADF.  
- **imagens/adf_datasets.png** â†’ Print da configuraÃ§Ã£o dos Datasets no ADF.  
- **arquitetura_azure.png** â†’ Diagrama da arquitetura do projeto (ADF + IR + ADLS + Databricks).  
- **guia_instalacao_ir.md** â†’ Guia detalhado de instalaÃ§Ã£o e configuraÃ§Ã£o do Self-hosted IR.  

### ğŸ“ adf/linkedServices/
- **LS_SQL_OnPrem.json** â†’ Linked Service para conexÃ£o com SQL Server on-premises via IR.  
- **LS_ADLS.json** â†’ Linked Service para conexÃ£o com o Data Lake Storage Gen2.  
- **LS_KeyVault.json** â†’ Linked Service para acessar segredos armazenados no Azure Key Vault.  

### ğŸ“ adf/datasets/
- **DS_SQL_OnPrem_<Tabela>.json** â†’ Dataset de origem (tabela SQL on-premises).  
- **DS_ADLS_RAW_TXT.json** â†’ Dataset de destino na camada `raw` (arquivos TXT/CSV).  
- **DS_ADLS_BRONZE_TXT.json** â†’ Dataset de destino na camada `bronze`.  

### ğŸ“ adf/pipelines/
- **pl_redundancia_sql_to_datalake.json** â†’ Pipeline principal que copia dados do SQL para o Data Lake (`raw` â†’ `bronze`).  

### ğŸ“ adf/triggers/
- **tr_daily_0200_brt.json** â†’ Trigger de execuÃ§Ã£o diÃ¡ria Ã s 02:00 BRT.  

### ğŸ“ databricks/
- **notebooks/bronze_promote_<tabela>.ipynb** â†’ Notebook Databricks para promover dados da camada `raw` para `bronze`.  
- **configs/cluster_config.json** â†’ ConfiguraÃ§Ã£o de cluster Databricks (nÃ³s, versÃ£o Spark, auto-terminaÃ§Ã£o).  

### ğŸ“ scripts/
- **sql/create_sample_table.sql** â†’ Script SQL para criar tabela de exemplo e inserir dados.  
- **powershell/install_self_hosted_ir.ps1** â†’ Script PowerShell para instalar o Self-hosted IR.  

### ğŸ“ logs/
- **samples/run_metadata_example.json** â†’ Exemplo de log de execuÃ§Ã£o de pipeline (metadata: tempo, status, registros copiados).  

---

## â–¶ï¸ Como Executar o Projeto

1. **PreparaÃ§Ã£o no Azure**
   - Crie um **Resource Group**.  
   - Crie um **Storage Account** com **Data Lake Gen2** habilitado.  
   - Crie um **Data Factory**.  

2. **InstalaÃ§Ã£o do Self-hosted IR**
   - Siga o guia em [`docs/guia_instalacao_ir.md`](docs/guia_instalacao_ir.md).  
   - Instale o IR na mÃ¡quina local e registre com a chave do ADF.  

3. **ConfiguraÃ§Ã£o no ADF**
   - Importe os **Linked Services** (`LS_SQL_OnPrem.json`, `LS_ADLS.json`, `LS_KeyVault.json`).  
   - Importe os **Datasets** (`DS_SQL_OnPrem_<Tabela>.json`, `DS_ADLS_RAW_TXT.json`, `DS_ADLS_BRONZE_TXT.json`).  
   - Importe o **Pipeline** (`pl_redundancia_sql_to_datalake.json`).  
   - Configure o **Trigger** (`tr_daily_0200_brt.json`).  

4. **ExecuÃ§Ã£o**
   - Execute manualmente o pipeline ou aguarde o trigger diÃ¡rio.  
   - Verifique os arquivos gerados no ADLS (`raw` e `bronze`).  

5. **Processamento com Databricks (opcional)**
   - Configure o cluster com `databricks/configs/cluster_config.json`.  
   - Execute o notebook `bronze_promote_<tabela>.ipynb` para promover dados.  

6. **ValidaÃ§Ã£o**
   - Consulte os logs em `logs/samples/run_metadata_example.json`.  
   - Verifique prints em `docs/imagens/` para confirmar configuraÃ§Ã£o.  

---

## ğŸ“¸ Prints do Projeto

- Linked Services â†’ `docs/imagens/adf_linked_services.png`  
- Datasets â†’ `docs/imagens/adf_datasets.png`  
- Arquitetura â†’ `docs/arquitetura_azure.png`  

---

## ğŸ›¡ï¸ Boas PrÃ¡ticas
- **SeguranÃ§a:** Armazene segredos no Key Vault.  
- **GovernanÃ§a:** Organize dados em camadas (`raw`, `bronze`).  
- **Performance:** Ajuste paralelismo no Copy Activity.  
- **Custos:** Use redundÃ¢ncia LRS em conta de estudante.  
- **Logs:** Sempre registre metadata de execuÃ§Ã£o.  

---

## ğŸ“œ LicenÃ§a
Este projeto estÃ¡ licenciado sob a licenÃ§a MIT.  
Sinta-se livre para usar e adaptar em seus prÃ³prios projetos.

---

## âœ¨ ConclusÃ£o
Este projeto demonstra uma soluÃ§Ã£o prÃ¡tica e didÃ¡tica para **redundÃ¢ncia de arquivos na Azure**, integrando ambientes locais e nuvem. 

Com pipelines bem estruturados, camadas de dados e documentaÃ§Ã£o completa, vocÃª terÃ¡ um portfÃ³lio sÃ³lido para apresentar em entrevistas e projetos reais.


